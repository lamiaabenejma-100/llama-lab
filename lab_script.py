# lab_script.py
"""
Lab Assignment 3 - Practical Introduction to LLaMA Models
Version avec demande interactive du token si n√©cessaire
"""

# =============== AJOUTEZ CES LIGNES POUR FIX SSL ===============
import os
import ssl
import warnings
import sys

# Fix pour les probl√®mes de certificat SSL
ssl._create_default_https_context = ssl._create_unverified_context
os.environ['CURL_CA_BUNDLE'] = ''
os.environ['REQUESTS_CA_BUNDLE'] = ''
os.environ['HF_HUB_DISABLE_SSL_VERIFICATION'] = '1'
warnings.filterwarnings("ignore")
# ================================================================

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from huggingface_hub import login

# ----------------------------
# 0. FONCTION POUR DEMANDER LE TOKEN INTERACTIVEMENT
# ----------------------------
def get_hf_token_interactive():
    """Demande le token √† l'utilisateur de mani√®re interactive"""
    
    print("\n" + "=" * 60)
    print("üîê AUTHENTIFICATION HUGGING FACE")
    print("=" * 60)
    
    print("\nPour utiliser LLaMA 3.2, vous avez besoin d'un token Hugging Face.")
    print("\nSi vous n'avez pas de token, vous pouvez:")
    print("1. Utiliser un mod√®le open-source (appuyez sur Entr√©e)")
    print("2. Ou obtenir un token sur: https://huggingface.co/settings/tokens")
    
    choice = input("\nVoulez-vous entrer un token? (o/N): ").strip().lower()
    
    if choice == 'o' or choice == 'oui':
        print("\n" + "-" * 40)
        print("INSTRUCTIONS:")
        print("1. Allez sur: https://huggingface.co/settings/tokens")
        print("2. Cr√©ez un nouveau token (niveau 'read')")
        print("3. Copiez le token (commence par 'hf_')")
        print("-" * 40)
        
        token = input("\nEntrez votre token Hugging Face: ").strip()
        
        # V√©rification basique
        if token and token.startswith('hf_'):
            # Option: sauvegarder dans .env
            save = input("\nVoulez-vous sauvegarder dans .env pour la prochaine fois? (o/N): ").strip().lower()
            if save == 'o' or save == 'oui':
                with open(".env", "w") as f:
                    f.write(f"HUGGING_FACE_HUB_TOKEN={token}")
                print("‚úÖ Token sauvegard√© dans .env")
            
            return token
        else:
            print("‚ö†Ô∏è  Token invalide ou vide. Utilisation d'un mod√®le open-source.")
            return None
    else:
        print("‚úÖ Utilisation d'un mod√®le open-source (pas de token requis)")
        return None

# ----------------------------
# 1. R√âCUP√âRATION DU TOKEN
# ----------------------------
HF_TOKEN = None
USE_LLAMA = False

# Essayer d'abord les sources automatiques
try:
    from dotenv import load_dotenv
    load_dotenv()
    HF_TOKEN = os.getenv("HUGGING_FACE_HUB_TOKEN")
    if HF_TOKEN:
        print("‚úÖ Token trouv√© dans .env")
        USE_LLAMA = True
except:
    pass

# Si pas de token, demander interactivement
if not HF_TOKEN:
    HF_TOKEN = get_hf_token_interactive()
    if HF_TOKEN:
        USE_LLAMA = True

# Authentification si token disponible
if USE_LLAMA and HF_TOKEN:
    try:
        login(token=HF_TOKEN)
        print("‚úÖ Authentification Hugging Face r√©ussie")
    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur d'authentification: {e}")
        print("Utilisation d'un mod√®le open-source √† la place...")
        USE_LLAMA = False

# ----------------------------
# 2. SETUP & MODEL OVERVIEW
# ----------------------------
print("\n" + "=" * 60)
print("1. SETUP & MODEL OVERVIEW")
print("=" * 60)

# Choix du mod√®le selon l'acc√®s
if USE_LLAMA:
    MODEL_NAME = "meta-llama/Llama-3.2-1B"
    print(f"üéØ Mod√®le LLaMA s√©lectionn√©: {MODEL_NAME}")
else:
    # Mod√®le open-source alternatif
    MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    print(f"üîì Mod√®le open-source s√©lectionn√©: {MODEL_NAME}")

print(f"\nChargement du mod√®le: {MODEL_NAME}")

try:
    # Configuration de quantisation
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True
    )
    
    # Charger le tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    tokenizer.pad_token = tokenizer.eos_token
    
    # Charger le mod√®le avec quantisation
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        quantization_config=bnb_config,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
    print("‚úÖ Mod√®le charg√© avec succ√®s!")
    print(f"\nüìä Informations du mod√®le:")
    print(f"   ‚Ä¢ Architecture: {model.config.model_type}")
    print(f"   ‚Ä¢ Taille du vocabulaire: {model.config.vocab_size:,}")
    print(f"   ‚Ä¢ Device: {model.device}")
    
except Exception as e:
    print(f"‚ùå Erreur lors du chargement: {e}")
    print("\nTentative avec un mod√®le plus l√©ger...")
    
    # Fallback ultra-l√©ger
    MODEL_NAME = "microsoft/phi-2"
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    print(f"‚úÖ Mod√®le de secours {MODEL_NAME} charg√©!")

# ----------------------------
# 3. BASIC INFERENCE & PROMPTING
# ----------------------------
print("\n" + "=" * 60)
print("2. BASIC INFERENCE & PROMPTING - Strat√©gies de d√©codage")
print("=" * 60)

# Fonction pour formater le prompt selon le mod√®le
def format_prompt(user_message):
    if "llama" in MODEL_NAME.lower():
        return f"""<|begin_of_text|><|start_header_id|>user<|end_header_id|>

{user_message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
    elif "phi" in MODEL_NAME.lower():
        return f"Instruct: {user_message}\nOutput:"
    else:
        return f"<|user|>\n{user_message}\n<|assistant|>\n"

prompt_text = "Explain transformers (the AI model) to a 12-year-old."
prompt = format_prompt(prompt_text)

print("Prompt original:")
print(f"  '{prompt_text}'")
print("\nPrompt format√©:")
print(f"  '{prompt[:80]}...'")

# Encodage
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# Tests des diff√©rentes strat√©gies de d√©codage
strategies = {
    "Greedy Decoding": {
        "do_sample": False,
        "max_new_tokens": 100
    },
    "Sampling (temp=0.7)": {
        "do_sample": True,
        "temperature": 0.7,
        "max_new_tokens": 100
    },
    "Top-k Sampling (k=50)": {
        "do_sample": True,
        "top_k": 50,
        "max_new_tokens": 100
    }
}

for strategy_name, params in strategies.items():
    print(f"\n{'‚îÄ' * 40}")
    print(f"üéØ {strategy_name}")
    print(f"{'‚îÄ' * 40}")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            **params,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # D√©codage et affichage
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # Extraire seulement la r√©ponse apr√®s le prompt
    response_text = response[len(prompt):].strip()
    print(response_text[:250] + "..." if len(response_text) > 250 else response_text)

# ----------------------------
# 4. PROMPT ENGINEERING
# ----------------------------
print("\n" + "=" * 60)
print("3. PROMPT ENGINEERING - Techniques avanc√©es")
print("=" * 60)

# a) Zero-shot
zero_shot_prompt = format_prompt("Is the following statement true or false? The Earth orbits the Sun.")
print("\nüîç Zero-shot Prompting:")
inputs = tokenizer(zero_shot_prompt, return_tensors="pt").to(model.device)
output = model.generate(**inputs, max_new_tokens=50, do_sample=False)
response = tokenizer.decode(output[0], skip_special_tokens=True)
print(response.split("assistant")[-1].strip() if "assistant" in response else response[-150:])

# b) One-shot
one_shot_text = """Q: What is the capital of France?
A: Paris

Q: What is the capital of Germany?
A:"""
one_shot_prompt = format_prompt(one_shot_text)
print("\nüéØ One-shot Prompting:")
inputs = tokenizer(one_shot_prompt, return_tensors="pt").to(model.device)
output = model.generate(**inputs, max_new_tokens=20, do_sample=False)
response = tokenizer.decode(output[0], skip_special_tokens=True)
answer = response.split("A:")[-1].strip().split("\n")[0]
print(f"R√©ponse: {answer}")

# c) Fact-checking avec sortie structur√©e
fact_text = """Fact-check the following statement: "The Moon is made of cheese."
Provide your answer in this format:
- Statement: [original statement]
- Truthfulness: [true/false/partially true]
- Explanation: [brief explanation]"""

fact_prompt = format_prompt(fact_text)
print("\n‚úÖ Fact-checking with Structured Output:")
inputs = tokenizer(fact_prompt, return_tensors="pt").to(model.device)
output = model.generate(**inputs, max_new_tokens=150, do_sample=True, temperature=0.3)
response = tokenizer.decode(output[0], skip_special_tokens=True)
fact_response = response.split("assistant")[-1].strip() if "assistant" in response else response[-300:]
print(fact_response[:300])

print("\n" + "=" * 60)
print("‚úÖ LAB SCRIPT TERMIN√â AVEC SUCC√àS!")
print("=" * 60)