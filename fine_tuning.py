# fine_tuning.py
"""
Fine-tuning LLaMA 3.2-1B with LoRA/QLoRA
Parameter-efficient fine-tuning for sentiment analysis
"""

import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
from huggingface_hub import login
import os
from dotenv import load_dotenv
import numpy as np

# ----------------------------
# CONFIGURATION
# ----------------------------
load_dotenv()
HF_TOKEN = os.getenv("HUGGING_FACE_HUB_TOKEN")

if not HF_TOKEN:
    print("‚ùå Token Hugging Face non trouv√©!")
    exit(1)

login(token=HF_TOKEN)

# Mod√®le LLaMA 3.2
MODEL_NAME = "meta-llama/Llama-3.2-1B"
DATASET_NAME = "imdb"
OUTPUT_DIR = "./llama3.2-finetuned-lora"

# Param√®tres LoRA
LORA_R = 8
LORA_ALPHA = 16
LORA_DROPOUT = 0.1

# Param√®tres d'entra√Ænement
EPOCHS = 1
BATCH_SIZE = 2
GRAD_ACCUM = 4
LEARNING_RATE = 2e-4

print("=" * 60)
print("FINE-TUNING LLaMA 3.2-1B WITH LoRA")
print("=" * 60)

# ----------------------------
# 1. CHARGEMENT DU MOD√àLE ET TOKENIZER
# ----------------------------
print(f"\n1. Chargement du mod√®le {MODEL_NAME}...")

# Configuration de quantisation 4-bit
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_storage=torch.float16
)

try:
    # Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)
    tokenizer.pad_token = tokenizer.eos_token
    
    # Mod√®le avec quantisation
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        quantization_config=bnb_config,
        torch_dtype=torch.float16,
        device_map="auto",
        token=HF_TOKEN
    )
    
    # Pr√©parer le mod√®le pour l'entra√Ænement k-bit
    model = prepare_model_for_kbit_training(model)
    
    print("‚úÖ Mod√®le charg√© avec succ√®s!")
    
except Exception as e:
    print(f"‚ùå Erreur: {e}")
    print("Utilisation d'un mod√®le alternatif...")
    MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        load_in_4bit=True,
        torch_dtype=torch.float16,
        device_map="auto"
    )

# ----------------------------
# 2. CONFIGURATION LoRA
# ----------------------------
print("\n2. Configuration LoRA...")

lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=LORA_R,
    lora_alpha=LORA_ALPHA,
    lora_dropout=LORA_DROPOUT,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    bias="none",
    modules_to_save=["lm_head", "embed_tokens"]
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# ----------------------------
# 3. PR√âPARATION DES DONN√âES
# ----------------------------
print(f"\n3. Chargement du dataset {DATASET_NAME}...")

# Charger le dataset IMDB
dataset = load_dataset(DATASET_NAME)

# Prendre un sous-ensemble pour l'entra√Ænement rapide
train_dataset = dataset["train"].select(range(1000))
eval_dataset = dataset["test"].select(range(200))

print(f"   ‚Ä¢ Exemples d'entra√Ænement: {len(train_dataset)}")
print(f"   ‚Ä¢ Exemples d'√©valuation: {len(eval_dataset)}")

# Fonction de pr√©paration des donn√©es
def preprocess_function(examples):
    # Cr√©er des prompts pour la classification de sentiments
    prompts = []
    for text, label in zip(examples["text"], examples["label"]):
        sentiment = "positive" if label == 1 else "negative"
        prompt = f"""<|begin_of_text|><|start_header_id|>user<|end_header_id|>

Analyze the sentiment of this movie review:

"{text[:500]}"

Sentiment:<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{sentiment}<|eot_id|>"""
        prompts.append(prompt)
    
    # Tokenizer
    tokenized = tokenizer(
        prompts,
        truncation=True,
        padding="max_length",
        max_length=512,
        return_tensors=None
    )
    
    # Les labels sont les input_ids pour le language modeling
    tokenized["labels"] = tokenized["input_ids"].copy()
    
    return tokenized

# Appliquer le pr√©processing
print("   ‚Ä¢ Pr√©processing des donn√©es...")
tokenized_train = train_dataset.map(
    preprocess_function,
    batched=True,
    batch_size=32,
    remove_columns=train_dataset.column_names
)

tokenized_eval = eval_dataset.map(
    preprocess_function,
    batched=True,
    batch_size=32,
    remove_columns=eval_dataset.column_names
)

print(f"   ‚Ä¢ Taille finale train: {len(tokenized_train)}")
print(f"   ‚Ä¢ Taille finale eval: {len(tokenized_eval)}")

# ----------------------------
# 4. CONFIGURATION DE L'ENTRA√éNEMENT
# ----------------------------
print("\n4. Configuration de l'entra√Ænement...")

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRAD_ACCUM,
    warmup_steps=50,
    logging_steps=25,
    eval_steps=100,
    save_steps=100,
    evaluation_strategy="steps",
    save_strategy="steps",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    learning_rate=LEARNING_RATE,
    fp16=True,
    optim="paged_adamw_8bit",
    report_to="none",
    save_total_limit=2,
    remove_unused_columns=False,
    push_to_hub=False,
)

# Data collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

# ----------------------------
# 5. ENTRA√éNEMENT
# ----------------------------
print("\n5. D√©but de l'entra√Ænement...")
print("   (Cela peut prendre quelques minutes)")

train_result = trainer.train()

# Sauvegarde
print(f"\n6. Sauvegarde du mod√®le dans {OUTPUT_DIR}...")
trainer.save_model()
tokenizer.save_pretrained(OUTPUT_DIR)

print(f"‚úÖ Entra√Ænement termin√©!")
print(f"   ‚Ä¢ Loss finale: {train_result.training_loss:.4f}")

# ----------------------------
# 6. √âVALUATION AVANT/APR√àS
# ----------------------------
print("\n7. √âvaluation avant/apr√®s fine-tuning")

def test_sentiment(text, model, tokenizer):
    """Teste le mod√®le sur un texte donn√©"""
    prompt = f"""<|begin_of_text|><|start_header_id|>user<|end_header_id|>

Analyze the sentiment of this movie review:

"{text}"

Sentiment:<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
    
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=10,
            do_sample=False,
            temperature=0.1,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    sentiment = response.split("assistant")[-1].strip().lower()
    
    return sentiment

# Textes de test
test_texts = [
    "This movie was absolutely fantastic! The acting was superb and the story was captivating.",
    "I was very disappointed with this film. The plot was weak and the characters were uninteresting.",
    "An average movie with some good moments but overall nothing special."
]

print("\nüìä R√©sultats des tests:")
for i, text in enumerate(test_texts, 1):
    print(f"\nTest {i}:")
    print(f"   Texte: {text[:80]}...")
    sentiment = test_sentiment(text, model, tokenizer)
    print(f"   Sentiment pr√©dit: {sentiment}")

print("\n" + "=" * 60)
print("‚úÖ FINE-TUNING TERMIN√â AVEC SUCC√àS")
print("=" * 60)